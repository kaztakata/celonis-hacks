<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Celonis hacks</title>
    <link>https://kaztakata.github.io/celonis-hacks/posts/</link>
    <description>Recent content in Posts on Celonis hacks</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 25 Jun 2022 05:05:00 +0900</lastBuildDate><atom:link href="https://kaztakata.github.io/celonis-hacks/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Run first Action Flow Scenario</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-06-25-run-first-action-flow-scenario/</link>
      <pubDate>Sat, 25 Jun 2022 05:05:00 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-06-25-run-first-action-flow-scenario/</guid>
      <description>Action Flow as HTTP client From today I would like to introduce Action Flow, that is possible to automate scenario and integrate SaaS systems (also on-premise too).
You may know that 100 over SaaS systems are registered in Action Flow and easily build your own scenario. Great, but I sometimes felt that I could not find appropriate module from that. How do I fullfill my requirement ?
Actually Action Flow can be used HTTP client, so what I did by cURL at last post Operate Celonis from outside by REST API is also possible in Action Flow.</description>
    </item>
    
    <item>
      <title>Operate Celonis from outside by REST API</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-06-18-operate-celonis-from-outside-by-rest-api/</link>
      <pubDate>Sat, 18 Jun 2022 17:59:00 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-06-18-operate-celonis-from-outside-by-rest-api/</guid>
      <description>About REST API Until last post, I explained some of API use cases (export Pool program and configure data job alert) using Pycelonis script from ML Workbench. Is is something Celonis &amp;ldquo;internal&amp;rdquo; operation. By the way, as I mentioned in Login to Celonis EMS from Jupyter Workbench, I can call API from outside of Celonis. Generally this kind of API using HTTP request is called REST API (or RESTful API)</description>
    </item>
    
    <item>
      <title>Include JSON data in HTTP Request and Response</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-06-11-include-json-data-in-http-request-and-response/</link>
      <pubDate>Sat, 11 Jun 2022 18:58:00 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-06-11-include-json-data-in-http-request-and-response/</guid>
      <description>About JSON Until last post, I did not intentionally mention about detail of input and output data in HTTP request. Actually HTTP request requires not only URL (URI) but header data like Authorization (refer to Observe HTTP request in Pycelonis login script). Also you can imagine it is possible to attach input form value (e.g. name, address, email). Generally in API world, JSON format is used to attach with HTTP request.</description>
    </item>
    
    <item>
      <title>Find out HTTP Request from GUI Function</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-06-04-find-out-http-request-from-gui-function/</link>
      <pubDate>Sat, 04 Jun 2022 09:25:43 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-06-04-find-out-http-request-from-gui-function/</guid>
      <description>In the previous post Observe HTTP request in Pycelonis login script, I showed how to observe HTTP request under Pycelonis API. In this observation I found HTTP request requires at least Authorization header and of course URL to reach to resource in Celonis EMS. Also I found that I can manage to investigate which HTTP request is sent when calling Pycelonis class method.
By the way, not all HTTP requests are implemented in Pycelonis.</description>
    </item>
    
    <item>
      <title>Observe HTTP request in Pycelonis login script</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-05-28-observe-http-request-in-pycelonis-login-script/</link>
      <pubDate>Sat, 28 May 2022 07:59:48 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-05-28-observe-http-request-in-pycelonis-login-script/</guid>
      <description>I showed how to login to Celonis EMS using Pycelonis in previous posts. Authentication topic I mentioned there is the most annoying when using API, but after mastering this I can transfer this knowledge to another areas easily.
To master authentication, I would like to show the mechanism of HTTP request in the internet (web) programming. Even I did not know HTTP request well at the beginning of using Celonis, now I got some of basic knowledge and it is enough to use HTTP request.</description>
    </item>
    
    <item>
      <title>Limit permissions of API token to minimize risk</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-05-21-limit-permissions-of-api-token-to-minimize-risk/</link>
      <pubDate>Sat, 21 May 2022 09:31:32 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-05-21-limit-permissions-of-api-token-to-minimize-risk/</guid>
      <description>At last post Login to Celonis EMS from Jupyter Workbench, I used API key that have same permission as my GUI user. I mentioned that it is too strong and risky against unauthorized access. Imagine your API token accidentally make public, then anyone can operate Celonis instead of you. That is why I segregate API token from Notebook (Notebook may be published to GitHub etc.). Anyway, user API token must be altered to another weaker key especially in production system.</description>
    </item>
    
    <item>
      <title>Login to Celonis EMS from Jupyter Workbench</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-05-14-login-to-celonis-ems-from-jupyter-workbench/</link>
      <pubDate>Sat, 14 May 2022 10:40:29 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-05-14-login-to-celonis-ems-from-jupyter-workbench/</guid>
      <description>At last post Start Deep Dive to Machine Learning and Action Flow, I introduced overview of these two functions that manipulate Celonis EMS functions without using GUI (browser) for automatation.
From today I would like to share basic functions in Machine Learning (Jupyter Workbench). Before staring I will setup Jupyter Workbench. If you do not have your Workbench, read Share my Analysis by Content-CLI then follow until creating Workbench.
I will go to Launcher in Workbench then select Others &amp;gt; Terminal, then enter below command after $ sign (output is as of 2022-05-14).</description>
    </item>
    
    <item>
      <title>Start Deep Dive to Machine Learning and Action Flow</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-05-07-start-deep-dive-to-machine-learning-and-action-flow/</link>
      <pubDate>Sat, 07 May 2022 12:54:54 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-05-07-start-deep-dive-to-machine-learning-and-action-flow/</guid>
      <description>Until last post, I have seen the flow of process discovery through Celonis Process Analytics (or Studio Analysis) and prerequisite ETL (Extraction, Transformation, Load) by Data Integration. I already worked in multiple static process mining projects and found that they are enough functions for static process mining. And other process mining solutions are provided these functions too.
By the way, referring to Process Mining Data Science in Action by Wil van der Aalst, Process Mining can also make it possible for further actions such as monitoring and predictive analysis.</description>
    </item>
    
    <item>
      <title>Execute Periodic ETL Automatically</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-04-30-execute-periodic-etl-automatically/</link>
      <pubDate>Sat, 30 Apr 2022 08:54:10 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-04-30-execute-periodic-etl-automatically/</guid>
      <description>Until last post Validate Data Model by Studio Analysis, I completed creating ETL programs in Data Integration. But this is still test product because this programs are executed manually by operator. In this post, I would like to share the final piece of Data Integration, how to run ETL programs automatically after production release.
There are several ways to achieve automatic ETL, for example in my production system I am using scheduler in Machine Learning Workbench to trigger Jupyter Notebook, then operate Data Integration via Pycelonis (Python API for Celonis EMS).</description>
    </item>
    
    <item>
      <title>Validate Data Model by Studio Analysis</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-04-23-validate-data-model-by-studio-analysis/</link>
      <pubDate>Sat, 23 Apr 2022 08:43:22 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-04-23-validate-data-model-by-studio-analysis/</guid>
      <description>At last post Construct My First Data Model, I created Data Model and load data to it. Normally initial load is not perfect, so I should check data in Data Model. Today I would like to share how to validate my Data Model using Analysis. By the way, Celonis EMS main function is now Studio (and App for viewer) and Analysis is also part of Studio, so today I will create Studio instead of Process Analytics to create Analysis.</description>
    </item>
    
    <item>
      <title>Construct My First Data Model</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-04-16-construct-my-first-data-model/</link>
      <pubDate>Sat, 16 Apr 2022 11:03:14 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-04-16-construct-my-first-data-model/</guid>
      <description>Today I would like to create draft version of data model then complete series of ETL (Extraction, Transformation, Load) started from last November.
As I showed in Adjust Time Zone of Event Time in Global Transformation, I created global VIEW for activity table. Before creating Data Model, I would like to create one more global VIEW against case table. As below SQL, it is quite simple to copy all columns of issues in Planio connection, plus one more column _CASE_KEY that is converting from interger id to character.</description>
    </item>
    
    <item>
      <title>Inspect Table Data by SELECT statement</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-04-09-inspect-table-data-by-select-statement/</link>
      <pubDate>Sat, 09 Apr 2022 17:40:32 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-04-09-inspect-table-data-by-select-statement/</guid>
      <description>Today I would like to share small tips in transformation tasks. Of course main purpose of transforamtion is to create event log table etc. but we are not always sure about source system behavior, so before completing transformation SQL I must investigate data of source system. As I already showed, I can use simple SELECT statement to inspect tables.
Previously when I was familiar with SAP ECC, the only way to easily inspect SAP table is to download table data by SE16 transaction and open Microsoft Excel and simply filter data or use pivot table etc.</description>
    </item>
    
    <item>
      <title>Adjust Time Zone of Event Time in Global Transformation</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-04-02-adjust-time-zone-of-event-time-in-global-transformation/</link>
      <pubDate>Sat, 02 Apr 2022 16:08:17 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-04-02-adjust-time-zone-of-event-time-in-global-transformation/</guid>
      <description>Until now I do not care about value of event time, that is stored at source system like Planio. If I analyse only one system like my current project it is no problem, but if I would like to analyze data from multiple source systems, I should take care time zone of each system.
In my case, I previously analyzed multiple SAP ECC systems, those are located in various area of world (US, EU, JP etc.</description>
    </item>
    
    <item>
      <title>Handle Day based Activity as Milestone</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-03-26-handle-day-based-activity-as-milestone/</link>
      <pubDate>Sat, 26 Mar 2022 01:48:53 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-03-26-handle-day-based-activity-as-milestone/</guid>
      <description>I already created four activities until last post Unite SQL statements by CASE Expression, those are fillfilled requirement of event log. Going back to Consider Case ID before Starting Transformation, case ID is the biggest requirement. Also is is not so big as case ID, but event time is important too. In process mining, event time should be year, month, day plus hour, minute, second (YYYY-MM-DD HH:MI:SS in Vertica format). I guess event time in process mining referred to that is recorded automatically by system responding to something action.</description>
    </item>
    
    <item>
      <title>Unite SQL statements by CASE Expression</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-03-19-unite-sql-statements-by-case-expression/</link>
      <pubDate>Sat, 19 Mar 2022 09:14:35 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-03-19-unite-sql-statements-by-case-expression/</guid>
      <description>Until Split Long SQL Using Views post I created four activities and each SQLs, and I found three of four have same table join pattern. So I created VIEW to shorten JOIN predicate for each SQL. It is nice to shorten total statement volume but almost all statement except for JOIN predicate is duplicated among SQLs. When something change is required, maintenance of each SQLs is annoying work. Today I would like to integrate SQLs using CASE expression.</description>
    </item>
    
    <item>
      <title>Split Long SQL Using Views</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-03-12-split-long-sql-using-views/</link>
      <pubDate>Sat, 12 Mar 2022 16:28:51 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-03-12-split-long-sql-using-views/</guid>
      <description>At last post I wrote Compose Activity from Joining Multiple Tables to create second activity Close Issue. Final version of SQL statement was long even I just used three tables. In real process mining project I handled hundred of tables and wrote quite long SQLs. At that time I faced same patterns of SQL in multiple activities. So I introduces VIEW in my SQLs for grouping same pattern of SELECT SQL, similar to create function (method) in programming.</description>
    </item>
    
    <item>
      <title>Compose Activity from Joining Multiple Tables</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-03-05-compose-activity-from-joining-multiple-tables/</link>
      <pubDate>Sat, 05 Mar 2022 08:30:59 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-03-05-compose-activity-from-joining-multiple-tables/</guid>
      <description>In the last post of Insert Simple Record into Activity Table, I created SQL to insert Raise Issue activity. That SQL was simple because only one table issues are used as data source. Today I will create SQL of Close Issue activity that requires multiple tables.
Same as previous post, first I should analyze columns to compose Close Issue activity. This is happend when I change Status column in each issue, so I need change history.</description>
    </item>
    
    <item>
      <title>Insert Simple Record into Activity Table</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-02-26-insert-simple-record-into-activity-table/</link>
      <pubDate>Sat, 26 Feb 2022 09:14:12 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-02-26-insert-simple-record-into-activity-table/</guid>
      <description>In the last post of Determine Process Mining Tables based on Project Goal, I talked strategy of transforming Planio data to event log. Then I wrote SQL to create activity table.
From now I would like to insert activity record to that table. Based on the discussion of last post, at least I need two activities Raise Issue and Close Issue to measure throughput time between them. That is the next step to achieve my goal.</description>
    </item>
    
    <item>
      <title>Determine Process Mining Tables based on Project Goal</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-02-19-determine-process-mining-tables-based-on-project-goal/</link>
      <pubDate>Sat, 19 Feb 2022 13:43:41 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-02-19-determine-process-mining-tables-based-on-project-goal/</guid>
      <description>I talked about how to consider Case ID in advance in last post. That is same as goal setting of process mining project, it means what would like to be measured in process mining and why. Case is the unit of measuring performance and grouping activities.
Considering my case of Planio, for example I would like to measure 1) Throughput time from raising issue to closing it, 2) How many users are involved until closing issue.</description>
    </item>
    
    <item>
      <title>Consider Case ID before Starting Transformation</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-02-12-consider-case-id-before-starting-transformation/</link>
      <pubDate>Sat, 12 Feb 2022 13:03:00 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-02-12-consider-case-id-before-starting-transformation/</guid>
      <description>After long explanation of Extractor Builder, I can move to Transformation Topic from now, using Planio issue and change history. But before starting detail discussion, I would like to discuss general issues at first.
In process mining context, Transformation is procedure to generate event log table from source tables. Event Log or Event Data is the collection of case and its event (activity) with timestamp. Case ID can be associated with multiple activities in source system, in other word it is not possible to generate event log without case ID.</description>
    </item>
    
    <item>
      <title>Tune Endpoint Parameter Relevant to Delta Load</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-02-05-tune-endpoint-parameter-relevant-to-delta-load/</link>
      <pubDate>Sat, 05 Feb 2022 10:05:51 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-02-05-tune-endpoint-parameter-relevant-to-delta-load/</guid>
      <description>Until last post Setup Dependent Endpoint in Extractor Builder, I prepared endpoints of both Planio Issues and their journals. Today I would like to tackle final setup of extractor to deal with Delta Load option.
Referring to the Planio Documentation, updated_on column exists for filtering Issues. This timestamp column is updated when creating and updating relevant issue, so it is appropriate column for Delta Load. Open Celonis Extractor builder then go to 4 Define Endpoints.</description>
    </item>
    
    <item>
      <title>Setup Dependent Endpoint in Extractor Builder</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-01-29-setup-dependent-endpoint-in-extractor-builder/</link>
      <pubDate>Sat, 29 Jan 2022 09:17:16 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-01-29-setup-dependent-endpoint-in-extractor-builder/</guid>
      <description>In the last post Configure Endpoint for Suitable Extraction, I configured Endpoint in Extractor Builder to suit my business requirements, and still there are points to extract change history of issues, and to extract data by Delta Load option. Today I would like to setup regarding change history using Dependent Endpoint in Extractor Builder.
At first how do I extract change history of Planio Issue ? Again I looked at Planio Documentation and found I can get single issue with journals (meaning change history in Planio).</description>
    </item>
    
    <item>
      <title>Configure Endpoint for Suitable Extraction</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-01-22-configure-endpoint-for-suitable-extraction/</link>
      <pubDate>Sat, 22 Jan 2022 08:08:39 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-01-22-configure-endpoint-for-suitable-extraction/</guid>
      <description>In the last post Connect to Source System via REST API, I shared how to set up Extractor Builder and extracted Issue from Planio. It was shortest path to be avaiable for extraction job, so it is not enough for production job. Today I would like to configure Endpoint in Extractor Builder to resolve problems I experienced.
First problem I faced is upper limit of extraction data. Some day I found that I could not get issue record until 25.</description>
    </item>
    
    <item>
      <title>Connect to Source System via REST API</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-01-15-connect-to-source-system-via-rest-api/</link>
      <pubDate>Sat, 15 Jan 2022 10:27:09 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-01-15-connect-to-source-system-via-rest-api/</guid>
      <description>At previous post Prepare Source System to Generate Event Log, I prepared Planio as source system for this blog, and entered few events (create Issue, update Issue Status) to it. Now it is time to extract event log from Planio. As other SaaS solution do, Planio also has REST API to extract data from outside. Currently Celonis EMS has ability to extract from arbitrary system that has REST API, Extractor Builder.</description>
    </item>
    
    <item>
      <title>Prepare Source System to Generate Event Log</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-01-08-prepare-source-system-to-generate-event-log/</link>
      <pubDate>Sat, 08 Jan 2022 16:35:30 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-01-08-prepare-source-system-to-generate-event-log/</guid>
      <description>Until last post I explained extraction topics using own Postgres database. I think it is better to test extraction functions with changing database by yourself. But it is hard to manually input data record that is meaningful as event log.
From now on I will move to transformation topic. To explain this, I think it is required to prepare source system that has user interface, database and API to connect to Celonis EMS, to easily generate event log and extract it.</description>
    </item>
    
    <item>
      <title>Pay attention to Extract SAP Tables</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-12-25-pay-attention-to-extract-sap-tables/</link>
      <pubDate>Sat, 25 Dec 2021 08:53:48 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-12-25-pay-attention-to-extract-sap-tables/</guid>
      <description>Until last post I explained general topics of extraction task, adapted to all kind of source systems. Today I would like to focus on SAP ECC or S4HANA as source system and would like to tell you the SAP specific issues.
First issue is regarding source system itself. We would like to guarantee source system&amp;rsquo;s availability even if I connect Celonis EMS to that. So we may choose testing environment that is snapshot of production system, as source system that connect to Celonis EMS.</description>
    </item>
    
    <item>
      <title>Use Pseudonymized Column as Grouping Key</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-12-18-use-pseudonymized-column-as-grouping-key/</link>
      <pubDate>Sat, 18 Dec 2021 09:22:56 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-12-18-use-pseudonymized-column-as-grouping-key/</guid>
      <description>One of the biggest headache for data engineer like me is how to assure data security when extracting data. Especially personal information should be dealt sensitively, otherwise I may be punished by each region&amp;rsquo;s law (e.g. GDPR).
When I operate Celonis EMS, I try not to extract sensitive information from the beginning, for example I do not extract table of customer address (ADRC table in SAP etc.). But this information is sometimes effective for grouping key of counting case etc.</description>
    </item>
    
    <item>
      <title>Understand Delta Load Configuration Difference in Adding Column Scenario</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-12-11-understand-delta-load-configuration-difference-in-adding-column-scenario/</link>
      <pubDate>Sat, 11 Dec 2021 21:54:02 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-12-11-understand-delta-load-configuration-difference-in-adding-column-scenario/</guid>
      <description>Last time I showed behavior when I added new record then extracted that record by Delta Load (Verify Cloning Table Contents via Delta Load). Delta Load is effective way to minimize extraction effort, but it is not always applied. Today, it is continued from previous post, I would like to add column to cloned table and observe behavior of extraction task.
After starting system operation including database, normally system is changing its requirement and extend function and database etc.</description>
    </item>
    
    <item>
      <title>Verify Cloning Table Contents via Delta Load</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-12-04-verify-cloning-table-contents-via-delta-load/</link>
      <pubDate>Sat, 04 Dec 2021 12:49:44 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-12-04-verify-cloning-table-contents-via-delta-load/</guid>
      <description>Following last week&amp;rsquo;s Minimize Extraction Time by Delta Load Option, today I would like to insert new record to Postgres table then try Delta Load again to extract it. To do this, I will start from operating pgAdmin, that is already ready for my loal machine after docker-compose.
First step is to enter localhost:5050 to my browser, then at the login screen enter pgadmin@celonis.cloud as email and pgadmin as password then click login button.</description>
    </item>
    
    <item>
      <title>Minimize Extraction Time by Delta Load Option</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-11-27-minimize-extraction-time-by-delta-load-option/</link>
      <pubDate>Sat, 27 Nov 2021 09:51:19 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-11-27-minimize-extraction-time-by-delta-load-option/</guid>
      <description>Last week I extracted Postgres table and looked at the log to understand mechanism of data transfer. At that time I used Full Load option to extract data, that is to replace all table contents and schema to latest version. That is easiest way to synchronize tables between source system (Postgres) and Celonis, but it takes a lot of time to complete this task. So that I should also use second option Delta Load to minimize extraction time.</description>
    </item>
    
    <item>
      <title>Look at Data Transfer Process by Data Job Log</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-11-20-look-at-data-transfer-process-by-data-job-log/</link>
      <pubDate>Sat, 20 Nov 2021 08:39:24 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-11-20-look-at-data-transfer-process-by-data-job-log/</guid>
      <description>Last week I posted Connect to Celonis and Bring Back Instruction to look at how Extractor works to connect between Celonis and Postgres. This week I would like to extract data from Postgres and look at data transfer process by data job log.
In the Data Integration, I create new Data Job with Data Connection I created last week, then create new extraction task. In the next screen I add new table public.</description>
    </item>
    
    <item>
      <title>Connect to Celonis and Bring Back Instruction</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-11-13-connect-to-celonis-and-bring-back-instruction/</link>
      <pubDate>Sat, 13 Nov 2021 10:19:49 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-11-13-connect-to-celonis-and-bring-back-instruction/</guid>
      <description>From last week I started Data Integration series and posted Run Extractor on Your Local Machine to prepare for my Extractor and Postgres database. Today I will start using Extractor and show you the mechanism to extract data safely.
From this week I will start Extractor and Postgres by next two steps (same as last week).
 Open VS code and open terminal at celonis-postgres folder. Enter docker-compose up in VS code terminal.</description>
    </item>
    
    <item>
      <title>Run Extractor on Your Local Machine</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-11-06-run-extractor-on-your-local-machine/</link>
      <pubDate>Sat, 06 Nov 2021 09:59:30 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-11-06-run-extractor-on-your-local-machine/</guid>
      <description>From this week I would like to explain my experience regarding Data Integration functions (Extraction, Transformation, Load etc.). To do this, I try to create sample source systems and build code in Celonis training environment.
As first topic, I would like to explain on premise Extractor, that is in the middle between your source systems (SAP, Oracle etc.) and Celonis EMS and support transferring data. By the way, because I do not want to pay licence of source systems for this blog, I would like to use open source Postgres database.</description>
    </item>
    
    <item>
      <title>Copy Previous Value to Blank Period by RUNNING_SUM and RANGE_APPEND functions</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-10-30-copy-previous-value-to-blank-period-by-running-sum-and-range-append-functions/</link>
      <pubDate>Sat, 30 Oct 2021 09:17:48 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-10-30-copy-previous-value-to-blank-period-by-running-sum-and-range-append-functions/</guid>
      <description>At Investigate Workload Trend of Cropped Subprocess I showed trend of activity count, and at that time I used RANGE_APPEND to fill zero count in trend graph. Today I would like to use different aggregation RUNNING_SUM and fill value to blank period.
Imagine you would like to check weekly trend of credit amount regarding some customer. Credit amount is increased by the amount of net value when &amp;lsquo;Receive Order&amp;rsquo; happened, and it is decreased when &amp;lsquo;Clear Invoice&amp;rsquo; happened.</description>
    </item>
    
    <item>
      <title>Create Key Column of Activity Table</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-10-23-create-key-column-of-activity-table/</link>
      <pubDate>Sat, 23 Oct 2021 08:02:29 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-10-23-create-key-column-of-activity-table/</guid>
      <description>Celonis Data Model always require unique key in case tabel (case key) to group activities belong to each case. How about activity table ? Activity table do not have explicit key column, instead combination of case key, activity name, timestamp, and sorting number are similar to activity key (those four columns are configured in Data Model).
This week I was asked to create activity table key column due to duplication check purpose.</description>
    </item>
    
    <item>
      <title>Investigate Workload Trend of Cropped Subprocess</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-10-16-investigate-workload-trend-of-cropped-subprocess/</link>
      <pubDate>Sat, 16 Oct 2021 10:12:18 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-10-16-investigate-workload-trend-of-cropped-subprocess/</guid>
      <description>Celonis Activity record is good to investigate user workload, so you may already implement analysis to do this. Today I would like to crop activities to minimum subprocess then investigate workload against subprocess.
Today&amp;rsquo;s output image is below. I set filter button of company code, also set three buttons to point out activities to crop subprocess pass through these activities. Right side Process Explorer is to check subprocess, in this case it is starting from &amp;lsquo;Approve Credit Check&amp;rsquo; until &amp;lsquo;Cancel Order&amp;rsquo; via &amp;lsquo;Deny Credit Check&amp;rsquo;.</description>
    </item>
    
    <item>
      <title>Verify calculation result in OLAP table then convert to visual component</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-10-09-verify-calculation-result-in-olap-table-then-convert-to-visual-component/</link>
      <pubDate>Sat, 09 Oct 2021 09:33:43 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-10-09-verify-calculation-result-in-olap-table-then-convert-to-visual-component/</guid>
      <description>Today is 24th post of this blog series and I am suprised that I can continue to post blog every week. I got some reply from who loves Celonis in the world and it is my fun to continue posting. Celonis is releasing new functionarity year by year and I am interested in catching up them and thinking how these functions help my clients. After listening to next week&amp;rsquo;s Celonis World Tour Webiner in Tokyo, I will get more inspiration to write blog.</description>
    </item>
    
    <item>
      <title>Group similar cases by Clustering</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-10-02-group-similar-cases-by-clustering/</link>
      <pubDate>Sat, 02 Oct 2021 10:56:43 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-10-02-group-similar-cases-by-clustering/</guid>
      <description>Last week I posted Convert Quantitative value to Categorical one by Quantile Function to create categorical dimension. Today I will create categorical dimension via different way, clustering.
Clustering is one of the unsupervised learning method, to automatically group cases by their attributes. Celonis PQL has clustering funciton KMEANS, so you are ready to use clustering.
Today I will use O2C process and would like to group customers by (1) their lead time from Invoice send to Clear Invoice and (2) net value.</description>
    </item>
    
    <item>
      <title>Convert Quantitative value to Categorical one by Quantile Function</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-09-25-convert-quantitative-value-to-categorical-one-by-quantile-function/</link>
      <pubDate>Sat, 25 Sep 2021 09:42:52 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-09-25-convert-quantitative-value-to-categorical-one-by-quantile-function/</guid>
      <description>Last week I posted Integrate Button Dropdown Entries to one Formula and integrated multiple dimensions. At that time I used string column that enable you to categorize each case, that is called categorical variable. In contrast quantitative variable such as order quantity, net value is not normally possible to use for dimension. If you would like to use these column as dimension, you need to convert its value to categorical value.</description>
    </item>
    
    <item>
      <title>Integrate Button Dropdown Entries to one Formula</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-09-18-integrate-button-dropdown-entries-to-one-formula/</link>
      <pubDate>Sat, 18 Sep 2021 10:42:24 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-09-18-integrate-button-dropdown-entries-to-one-formula/</guid>
      <description>I usually use two types of analysis components, time scale graph and drilldown table for my development projects. These compnents makes it possible to discover root cause of target KPIs by changing time scale or drilldown dimension.
In the end, these components and attaching button dropdown were maintained many times, but I was annoyed to set variable value (especially long PQL) to each button entry (it was also cause of defects against my analysis sheet).</description>
    </item>
    
    <item>
      <title>Use BIND function to enable multiple DOMAIN_TABLE</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-09-11-use-bind-function-to-enable-multiple-domain-table/</link>
      <pubDate>Sat, 11 Sep 2021 13:16:26 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-09-11-use-bind-function-to-enable-multiple-domain-table/</guid>
      <description>Variant Explorer is the major Celonis Analytical view to find process pattern by GUI operation. Of course I used it many times and enjoyed it at first, and found that I had a lot of effort to filter on and off to observe process. Today I would like to create collective list of variant to see major process KPI. Also I would like to show you BIND function that is difficult to understand but quite convenient if you know it.</description>
    </item>
    
    <item>
      <title>Convert count unit of KPI by COUNT DISTINCT</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-09-04-convert-count-unit-of-kpi-by-count-distinct/</link>
      <pubDate>Sat, 04 Sep 2021 09:06:11 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-09-04-convert-count-unit-of-kpi-by-count-distinct/</guid>
      <description>Last week I was asked to convert count unit of some KPI (that returns 1 or 0) from delivery item to delivery document (convert if all items in the document are 1 then document KPI is 1, else 0). In this case delivery document and item columns are stored at Activity table like this post&amp;rsquo;s third topic. I already used DOMAIN_TABLE to group activity record by delivery item then calculate KPI.</description>
    </item>
    
    <item>
      <title>Count rows of Tables in various way</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-08-28-count-rows-of-tables-in-various-way/</link>
      <pubDate>Sat, 28 Aug 2021 11:11:45 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-08-28-count-rows-of-tables-in-various-way/</guid>
      <description>Counting rows of tables is frequently used so we are not aware about how to do it. But sometimes I got stuck to do it so I would like to deep dive this topic today.
For this demo case I use P2P data model and three tables, header EKKO (key columns are MANDT/EBELN), item EKPO (MANDT/EBELN/EBELP) and activity. Also I used sum of item Net value. Below screen is the result of this demo.</description>
    </item>
    
    <item>
      <title>Make Conditional Function to return 1 or NULL</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-08-21-make-conditional-function-to-return-1-or-null/</link>
      <pubDate>Sat, 21 Aug 2021 10:42:17 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-08-21-make-conditional-function-to-return-1-or-null/</guid>
      <description>Previously I posted Handle NULL efficiently in Aggregation Function and discussed how to unite formula of COUNT, SUM, AVG functions. Today I will create boolean function and use it for conditional aggregation.
Imagine you would like to calculate net value of Sales Order (VBAP.NETWR_CONVERTED) in O2C process, but there are some conditons to calculate it. First condition is that sales order is active, means rejection reason column (VBAP.ABGRU) is not set.</description>
    </item>
    
    <item>
      <title>Share my Analysis by Content-CLI</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-08-14-share-my-analysis-by-content-cli/</link>
      <pubDate>Sat, 14 Aug 2021 13:33:06 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-08-14-share-my-analysis-by-content-cli/</guid>
      <description>Recently I was thinking how to share my work with you not only blog post but experience by live environment. Today I would like to tell how to reproduce my Analysis to your environment by Content-CLI. Content-CLI is used to copy programs between Celonis environments via backup file. I created backup file and upload to public repository. So you can download it and reproduce my program in your environment.
First you need to open your training environment if you do not.</description>
    </item>
    
    <item>
      <title>Create Matrix of Throughput Time by Pivot Table</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-08-07-create-matrix-of-throughput-time-by-pivot-table/</link>
      <pubDate>Sat, 07 Aug 2021 14:07:40 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-08-07-create-matrix-of-throughput-time-by-pivot-table/</guid>
      <description>At the previous post of Customize Process Explorer, I showed how to determine throughput time between two activities. This KPI is visible when expanding Process Explorer, but font size of Process Explorer become smaller and smaller when expanding connections, so it is difficult to grasp overview of throughput time.
Until previous post, I usually used OLAP table to show the list of KPI value. Of course it is good enough to grasp overview, but today I used different component Pivot Table to show KPI.</description>
    </item>
    
    <item>
      <title>Create Additional Entry to Button Dropdown</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-07-31-create-additional-entry-to-button-dropdown/</link>
      <pubDate>Sat, 31 Jul 2021 10:00:39 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-07-31-create-additional-entry-to-button-dropdown/</guid>
      <description>I looked at the Celopeers post that had issue when variable input is blank (NULL) then PQL using this variable had error.
I already used work around below to skip FILTER execution if variable is null. But I also felt troublesome in two points. First is this is not officially documented so myself need to instruct to my colleagues. Second is more important, I would like to unfilter this selection if variable is not set, but there was no way to do it.</description>
    </item>
    
    <item>
      <title>Categorize and Name Activity</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-07-24-categorize-and-name-activity/</link>
      <pubDate>Sat, 24 Jul 2021 10:47:12 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-07-24-categorize-and-name-activity/</guid>
      <description>In the previous post Transform Source System Tables to Minimize Data Model Tables, I recommended to convert some kind of source system tables to Activity. Today I focus on Activity and would like to share my way how to categorize and name Activity.
First point is to split Activity name to two parts, more general part and detail part. For example, in SAP ECC or S4HANA Order to Cash process, general Activity name is Create Sales Order when data committed in VA01 transaction.</description>
    </item>
    
    <item>
      <title>Transform Source System Tables to Minimize Data Model Tables</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-07-17-transform-source-system-tables-to-minimize-data-model-tables/</link>
      <pubDate>Sat, 17 Jul 2021 13:24:10 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-07-17-transform-source-system-tables-to-minimize-data-model-tables/</guid>
      <description>In the last part of previous post Utilize N-M relationship between Activity and Dimension Tables, I said just mimicing source system&amp;rsquo;s table structure to data model is not useful for Celonis data model. Also I feel loading time to data model is exponentially long when one more table is added. So minimizing data model table is good practice especially for real time analytics.
In case of SAP Order to Cash (O2C) scenario, case table is sales order item (VBAP).</description>
    </item>
    
    <item>
      <title>Utilize N-M relationship between Activity and Dimension Tables</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-07-10-utilize-n-m-relationship-between-activity-and-dimension-tables/</link>
      <pubDate>Sat, 10 Jul 2021 10:08:50 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-07-10-utilize-n-m-relationship-between-activity-and-dimension-tables/</guid>
      <description>When you determine data model structure by yourself, basically you should follow snowflake schema writtern in Understand how Tables are joined in Data Model. From case table perspective, n side is case table, and 1 side is another dimension table. On the other hand, activity table is the first case that 1 side is case table. If you determine second dimension table that 1 side is case table, please note that you can not use both activity and that dimension table at once because those tables are N-M relationship via case table.</description>
    </item>
    
    <item>
      <title>Maintain Saved Formulas effectively</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-07-03-maintain-saved-formulas-effectively/</link>
      <pubDate>Sat, 03 Jul 2021 21:25:28 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-07-03-maintain-saved-formulas-effectively/</guid>
      <description>In the last post Handle NULL efficiently in Aggregation Function, I used saved formulas to split long and complex PQL to reusable components. Today I would like to share my best practice to use saved formulas.
For example I would like to ananlyze throughput time between arbitrary two activities. As below screenshot I created three dropdown buttons, switching time unit (sec, min, hour, day etc.) and two activities (from / to).</description>
    </item>
    
    <item>
      <title>Handle NULL efficiently in Aggregation Function</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-06-26-handle-null-efficiently-in-aggregation-function/</link>
      <pubDate>Sat, 26 Jun 2021 09:37:59 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-06-26-handle-null-efficiently-in-aggregation-function/</guid>
      <description>I looked at many PQLs that can be simplified if they know about NULL handling well. Today I would like to tell how to handle NULL efficiently in Aggregation Functions (COUNT,SUM,AVG etc.).
Today I would like to use O2C process to explain my case. I determined KPI Send Invoice within a day after Ship Goods, because sales company will Ship Goods then should Send Invoice immediately.
Same as previous posts, first I would like to create OLAP table to look into the cases.</description>
    </item>
    
    <item>
      <title>Understand how Tables are joined in Data Model</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-06-19-understand-how-tables-are-joined-in-data-model/</link>
      <pubDate>Sat, 19 Jun 2021 11:42:09 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-06-19-understand-how-tables-are-joined-in-data-model/</guid>
      <description>In the Understand Difference between Dimension and KPI post, I mentioned that Columns used as dimensions and KPIs are inplicitly joined based on data model, but I did not mention how to join tables in data model. Today I would like to say about it.
Celonis Data Model support snowflake schema that consists of multiple tables, each table pair join as 1:N relationship. Normally activity table is top of N side and dimension tables including case table are bottom of 1 side as pyramid hierarchy.</description>
    </item>
    
    <item>
      <title>Calculate Multi Dimensional KPIs</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-06-12-calculate-multi-dimentional-kpis/</link>
      <pubDate>Sat, 12 Jun 2021 13:20:53 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-06-12-calculate-multi-dimentional-kpis/</guid>
      <description>In the last post Recognize Record to Calculate KPI, I showed issue of count duplication when I merged two different dimentional KPIs. Today I will tell how to calculate it correctly.
I will start from changing OLAP table to hide USER_TYPE and ACTIVITY_EN for calculating KPIs grouped by customer master. And I filtered by customer K1 as previous post. Finally I added SUM function to both Rework Time and Reminder Time.</description>
    </item>
    
    <item>
      <title>Recognize Record to Calculate KPI</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-06-05-recognize-record-to-calculate-kpi/</link>
      <pubDate>Sat, 05 Jun 2021 17:34:46 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-06-05-recognize-record-to-calculate-kpi/</guid>
      <description>Using Pull up function, you can calculate various kind of KPIs. I would like to tell in this post is taking care the record (dimensions) to calculate each KPI especially when you unite multiple KPIs to same component.
In this example, I will use Order to Cash process again and first I would like to estiamate time of rework. If rework Activity is operated by manual user, this rework is estimated 1 minute for example.</description>
    </item>
    
    <item>
      <title>Use Pull up function as Subquery</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-05-29-use-pull-up-function-as-subquery/</link>
      <pubDate>Sat, 29 May 2021 13:04:06 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-05-29-use-pull-up-function-as-subquery/</guid>
      <description>In the last post Determine First Time Right by Pull up function, I broke down PQL of FTR step by step. Today I will explain more complex KPI in similar way. By the way you may know Subquery that enables to pass result of SQL to parts of another SQL. PQL can also do similar things by Pull up function.
In this post, for example I would like to calculate Average rework count after Delivery in Order to Cash process, and definition of rework is same as previous post.</description>
    </item>
    
    <item>
      <title>Determine First Time Right by Pull up function</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-05-22-determine-first-time-right-by-pull-up-function/</link>
      <pubDate>Sat, 22 May 2021 16:31:16 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-05-22-determine-first-time-right-by-pull-up-function/</guid>
      <description>As a first example of Pull up function, I will show you First Time Right (FTR) that means process without rework activities (link).
To determine whether each case is FTR or not, first I should determine rework activities, then judge if each case has rework activities or not. In this example I determined that rework activity name is started from Change. PQL to find such a string pattern is Change %, using wildcard % after Change.</description>
    </item>
    
    <item>
      <title>Understand mechanism of Pull up function</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-05-15-understand-mechanism-of-pull-up-function/</link>
      <pubDate>Sat, 15 May 2021 10:00:14 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-05-15-understand-mechanism-of-pull-up-function/</guid>
      <description>Pull up aggregation function, called PU function, like PU_COUNT, PU_SUM, PU_AVG, PU_MAX, PU_MIN looks similar to Standard aggregation function like COUNT, SUM, AVG, MAX, MIN. In Celonis, it is important to understand that output of PU function is not KPI but dimension. You can see Understand Difference between Dimension and KPI.
If you know about SQL, you can imagine that PU function is similar to window function in SQL. As other functions, PU functions add dynamic column in grouping table.</description>
    </item>
    
    <item>
      <title>Customize Process Explorer</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-05-08-customize-process-explorer/</link>
      <pubDate>Sat, 08 May 2021 09:15:03 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-05-08-customize-process-explorer/</guid>
      <description>Process explorer in Celonis Analysis is default application to analyze process. First you can see main process pattern (called happy path), count of each step (activity) and count of path between two steps (connection). You can show minor activities and connections, or switch throughput time instead of count of connection.
I think there are two ways to customize Process explorer.
 Change activity name Add more KPI against activity / connection  Change activity name If you want to add detail information to each activity, you can change activity dimension.</description>
    </item>
    
    <item>
      <title>Understand Difference between Dimension and KPI</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-05-01-understand-difference-between-dimension-and-kpi/</link>
      <pubDate>Sat, 01 May 2021 15:25:08 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-05-01-understand-difference-between-dimension-and-kpi/</guid>
      <description>If you are beginner of creating Celonis Analysis, I believe you will first create OLAP table to understand how Celonis PQL works with data model. Points are,
 Dimensions are grouping key to calculate KPI, normally they are string (character) columns KPIs are calculated figures based on specific numeric columns, or count of rows of any columns, it must be numeric value in either cases Columns used as dimensions and KPIs are inplicitly joined based on data model  Below screenshot is the quite simple example to show Dimension, KPI, and data model.</description>
    </item>
    
  </channel>
</rss>
