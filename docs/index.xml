<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Celonis hacks</title>
    <link>https://kaztakata.github.io/celonis-hacks/</link>
    <description>Recent content on Celonis hacks</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 05 Mar 2022 08:30:59 +0900</lastBuildDate>
    
	<atom:link href="https://kaztakata.github.io/celonis-hacks/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Compose Activity from Joining Multiple Tables</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-03-05-compose-activity-from-joining-multiple-tables/</link>
      <pubDate>Sat, 05 Mar 2022 08:30:59 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-03-05-compose-activity-from-joining-multiple-tables/</guid>
      <description>In the last post of Insert Simple Record into Activity Table, I created SQL to insert Raise Issue activity. That SQL was simple because only one table issues are used as data source. Today I will create SQL of Close Issue activity that requires multiple tables.
Same as previous post, first I should analyze columns to compose Close Issue activity. This is happend when I change Status column in each issue, so I need change history.</description>
    </item>
    
    <item>
      <title>Insert Simple Record into Activity Table</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-02-26-insert-simple-record-into-activity-table/</link>
      <pubDate>Sat, 26 Feb 2022 09:14:12 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-02-26-insert-simple-record-into-activity-table/</guid>
      <description>In the last post of Determine Process Mining Tables based on Project Goal, I talked strategy of transforming Planio data to event log. Then I wrote SQL to create activity table.
From now I would like to insert activity record to that table. Based on the discussion of last post, at least I need two activities Raise Issue and Close Issue to measure throughput time between them. That is the next step to achieve my goal.</description>
    </item>
    
    <item>
      <title>Determine Process Mining Tables based on Project Goal</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-02-19-determine-process-mining-tables-based-on-project-goal/</link>
      <pubDate>Sat, 19 Feb 2022 13:43:41 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-02-19-determine-process-mining-tables-based-on-project-goal/</guid>
      <description>I talked about how to consider Case ID in advance in last post. That is same as goal setting of process mining project, it means what would like to be measured in process mining and why. Case is the unit of measuring performance and grouping activities.
Considering my case of Planio, for example I would like to measure 1) Throughput time from raising issue to closing it, 2) How many users are involved until closing issue.</description>
    </item>
    
    <item>
      <title>Consider Case ID before Starting Transformation</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-02-12-consider-case-id-before-starting-transformation/</link>
      <pubDate>Sat, 12 Feb 2022 13:03:00 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-02-12-consider-case-id-before-starting-transformation/</guid>
      <description>After long explanation of Extractor Builder, I can move to Transformation Topic from now, using Planio issue and change history. But before starting detail discussion, I would like to discuss general issues at first.
In process mining context, Transformation is procedure to generate event log table from source tables. Event Log or Event Data is the collection of case and its event (activity) with timestamp. Case ID can be associated with multiple activities in source system, in other word it is not possible to generate event log without case ID.</description>
    </item>
    
    <item>
      <title>Tune Endpoint Parameter Relevant to Delta Load</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-02-05-tune-endpoint-parameter-relevant-to-delta-load/</link>
      <pubDate>Sat, 05 Feb 2022 10:05:51 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-02-05-tune-endpoint-parameter-relevant-to-delta-load/</guid>
      <description>Until last post Setup Dependent Endpoint in Extractor Builder, I prepared endpoints of both Planio Issues and their journals. Today I would like to tackle final setup of extractor to deal with Delta Load option.
Referring to the Planio Documentation, updated_on column exists for filtering Issues. This timestamp column is updated when creating and updating relevant issue, so it is appropriate column for Delta Load. Open Celonis Extractor builder then go to 4 Define Endpoints.</description>
    </item>
    
    <item>
      <title>Setup Dependent Endpoint in Extractor Builder</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-01-29-setup-dependent-endpoint-in-extractor-builder/</link>
      <pubDate>Sat, 29 Jan 2022 09:17:16 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-01-29-setup-dependent-endpoint-in-extractor-builder/</guid>
      <description>In the last post Configure Endpoint for Suitable Extraction, I configured Endpoint in Extractor Builder to suit my business requirements, and still there are points to extract change history of issues, and to extract data by Delta Load option. Today I would like to setup regarding change history using Dependent Endpoint in Extractor Builder.
At first how do I extract change history of Planio Issue ? Again I looked at Planio Documentation and found I can get single issue with journals (meaning change history in Planio).</description>
    </item>
    
    <item>
      <title>Configure Endpoint for Suitable Extraction</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-01-22-configure-endpoint-for-suitable-extraction/</link>
      <pubDate>Sat, 22 Jan 2022 08:08:39 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-01-22-configure-endpoint-for-suitable-extraction/</guid>
      <description>In the last post Connect to Source System via REST API, I shared how to set up Extractor Builder and extracted Issue from Planio. It was shortest path to be avaiable for extraction job, so it is not enough for production job. Today I would like to configure Endpoint in Extractor Builder to resolve problems I experienced.
First problem I faced is upper limit of extraction data. Some day I found that I could not get issue record until 25.</description>
    </item>
    
    <item>
      <title>Connect to Source System via REST API</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-01-15-connect-to-source-system-via-rest-api/</link>
      <pubDate>Sat, 15 Jan 2022 10:27:09 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-01-15-connect-to-source-system-via-rest-api/</guid>
      <description>At previous post Prepare Source System to Generate Event Log, I prepared Planio as source system for this blog, and entered few events (create Issue, update Issue Status) to it. Now it is time to extract event log from Planio. As other SaaS solution do, Planio also has REST API to extract data from outside. Currently Celonis EMS has ability to extract from arbitrary system that has REST API, Extractor Builder.</description>
    </item>
    
    <item>
      <title>Prepare Source System to Generate Event Log</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-01-08-prepare-source-system-to-generate-event-log/</link>
      <pubDate>Sat, 08 Jan 2022 16:35:30 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-01-08-prepare-source-system-to-generate-event-log/</guid>
      <description>Until last post I explained extraction topics using own Postgres database. I think it is better to test extraction functions with changing database by yourself. But it is hard to manually input data record that is meaningful as event log.
From now on I will move to transformation topic. To explain this, I think it is required to prepare source system that has user interface, database and API to connect to Celonis EMS, to easily generate event log and extract it.</description>
    </item>
    
    <item>
      <title>Pay attention to Extract SAP Tables</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-12-25-pay-attention-to-extract-sap-tables/</link>
      <pubDate>Sat, 25 Dec 2021 08:53:48 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-12-25-pay-attention-to-extract-sap-tables/</guid>
      <description>Until last post I explained general topics of extraction task, adapted to all kind of source systems. Today I would like to focus on SAP ECC or S4HANA as source system and would like to tell you the SAP specific issues.
First issue is regarding source system itself. We would like to guarantee source system&amp;rsquo;s availability even if I connect Celonis EMS to that. So we may choose testing environment that is snapshot of production system, as source system that connect to Celonis EMS.</description>
    </item>
    
    <item>
      <title>Use Pseudonymized Column as Grouping Key</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-12-18-use-pseudonymized-column-as-grouping-key/</link>
      <pubDate>Sat, 18 Dec 2021 09:22:56 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-12-18-use-pseudonymized-column-as-grouping-key/</guid>
      <description>One of the biggest headache for data engineer like me is how to assure data security when extracting data. Especially personal information should be dealt sensitively, otherwise I may be punished by each region&amp;rsquo;s law (e.g. GDPR).
When I operate Celonis EMS, I try not to extract sensitive information from the beginning, for example I do not extract table of customer address (ADRC table in SAP etc.). But this information is sometimes effective for grouping key of counting case etc.</description>
    </item>
    
    <item>
      <title>Understand Delta Load Configuration Difference in Adding Column Scenario</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-12-11-understand-delta-load-configuration-difference-in-adding-column-scenario/</link>
      <pubDate>Sat, 11 Dec 2021 21:54:02 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-12-11-understand-delta-load-configuration-difference-in-adding-column-scenario/</guid>
      <description>Last time I showed behavior when I added new record then extracted that record by Delta Load (Verify Cloning Table Contents via Delta Load). Delta Load is effective way to minimize extraction effort, but it is not always applied. Today, it is continued from previous post, I would like to add column to cloned table and observe behavior of extraction task.
After starting system operation including database, normally system is changing its requirement and extend function and database etc.</description>
    </item>
    
    <item>
      <title>Verify Cloning Table Contents via Delta Load</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-12-04-verify-cloning-table-contents-via-delta-load/</link>
      <pubDate>Sat, 04 Dec 2021 12:49:44 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-12-04-verify-cloning-table-contents-via-delta-load/</guid>
      <description>Following last week&amp;rsquo;s Minimize Extraction Time by Delta Load Option, today I would like to insert new record to Postgres table then try Delta Load again to extract it. To do this, I will start from operating pgAdmin, that is already ready for my loal machine after docker-compose.
First step is to enter localhost:5050 to my browser, then at the login screen enter pgadmin@celonis.cloud as email and pgadmin as password then click login button.</description>
    </item>
    
    <item>
      <title>Minimize Extraction Time by Delta Load Option</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-11-27-minimize-extraction-time-by-delta-load-option/</link>
      <pubDate>Sat, 27 Nov 2021 09:51:19 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-11-27-minimize-extraction-time-by-delta-load-option/</guid>
      <description>Last week I extracted Postgres table and looked at the log to understand mechanism of data transfer. At that time I used Full Load option to extract data, that is to replace all table contents and schema to latest version. That is easiest way to synchronize tables between source system (Postgres) and Celonis, but it takes a lot of time to complete this task. So that I should also use second option Delta Load to minimize extraction time.</description>
    </item>
    
    <item>
      <title>Look at Data Transfer Process by Data Job Log</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-11-20-look-at-data-transfer-process-by-data-job-log/</link>
      <pubDate>Sat, 20 Nov 2021 08:39:24 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-11-20-look-at-data-transfer-process-by-data-job-log/</guid>
      <description>Last week I posted Connect to Celonis and Bring Back Instruction to look at how Extractor works to connect between Celonis and Postgres. This week I would like to extract data from Postgres and look at data transfer process by data job log.
In the Event Collection, I create new Data Job with Data Connection I created last week, then create new extraction task. In the next screen I add new table public.</description>
    </item>
    
    <item>
      <title>Connect to Celonis and Bring Back Instruction</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-11-13-connect-to-celonis-and-bring-back-instruction/</link>
      <pubDate>Sat, 13 Nov 2021 10:19:49 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-11-13-connect-to-celonis-and-bring-back-instruction/</guid>
      <description>From last week I started Event Collection series and posted Run Extractor on Your Local Machine to prepare for my Extractor and Postgres database. Today I will start using Extractor and show you the mechanism to extract data safely.
From this week I will start Extractor and Postgres by next two steps (same as last week).
 Open VS code and open terminal at celonis-postgres folder. Enter docker-compose up in VS code terminal.</description>
    </item>
    
    <item>
      <title>Run Extractor on Your Local Machine</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-11-06-run-extractor-on-your-local-machine/</link>
      <pubDate>Sat, 06 Nov 2021 09:59:30 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-11-06-run-extractor-on-your-local-machine/</guid>
      <description>From this week I would like to explain my experience regarding Event Collection functions (Extraction, Transformation, Load etc.). To do this, I try to create sample source systems and build code in Celonis training environment.
As first topic, I would like to explain on premise Extractor, that is in the middle between your source systems (SAP, Oracle etc.) and Celonis EMS and support transferring data. By the way, because I do not want to pay licence of source systems for this blog, I would like to use open source Postgres database.</description>
    </item>
    
    <item>
      <title>Copy Previous Value to Blank Period by RUNNING_SUM and RANGE_APPEND functions</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-10-30-copy-previous-value-to-blank-period-by-running-sum-and-range-append-functions/</link>
      <pubDate>Sat, 30 Oct 2021 09:17:48 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-10-30-copy-previous-value-to-blank-period-by-running-sum-and-range-append-functions/</guid>
      <description>At Investigate Workload Trend of Cropped Subprocess I showed trend of activity count, and at that time I used RANGE_APPEND to fill zero count in trend graph. Today I would like to use different aggregation RUNNING_SUM and fill value to blank period.
Imagine you would like to check weekly trend of credit amount regarding some customer. Credit amount is increased by the amount of net value when &amp;lsquo;Receive Order&amp;rsquo; happened, and it is decreased when &amp;lsquo;Clear Invoice&amp;rsquo; happened.</description>
    </item>
    
    <item>
      <title>Create Key Column of Activity Table</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-10-23-create-key-column-of-activity-table/</link>
      <pubDate>Sat, 23 Oct 2021 08:02:29 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-10-23-create-key-column-of-activity-table/</guid>
      <description>Celonis Data Model always require unique key in case tabel (case key) to group activities belong to each case. How about activity table ? Activity table do not have explicit key column, instead combination of case key, activity name, timestamp, and sorting number are similar to activity key (those four columns are configured in Data Model).
This week I was asked to create activity table key column due to duplication check purpose.</description>
    </item>
    
    <item>
      <title>About this site</title>
      <link>https://kaztakata.github.io/celonis-hacks/page/about/</link>
      <pubDate>Sun, 17 Oct 2021 22:42:10 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/page/about/</guid>
      <description>Greeting Hello, I am Kazuhiko Takata, called Kaz. I am working as freelance consultant from 2005, especially technical expert of SAP and Celonis. Please see my LinkedIn profile if you are interested.
Objective I would like to share what I learned to catch up latest Celonis technical topics and finally become Celonis Data Engineer. When I started Celonis career in 2019, I developed Celonis programs from scratch to analyze SAP process.</description>
    </item>
    
    <item>
      <title>Investigate Workload Trend of Cropped Subprocess</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-10-16-investigate-workload-trend-of-cropped-subprocess/</link>
      <pubDate>Sat, 16 Oct 2021 10:12:18 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-10-16-investigate-workload-trend-of-cropped-subprocess/</guid>
      <description>Celonis Activity record is good to investigate user workload, so you may already implement analysis to do this. Today I would like to crop activities to minimum subprocess then investigate workload against subprocess.
Today&amp;rsquo;s output image is below. I set filter button of company code, also set three buttons to point out activities to crop subprocess pass through these activities. Right side Process Explorer is to check subprocess, in this case it is starting from &amp;lsquo;Approve Credit Check&amp;rsquo; until &amp;lsquo;Cancel Order&amp;rsquo; via &amp;lsquo;Deny Credit Check&amp;rsquo;.</description>
    </item>
    
    <item>
      <title>Verify calculation result in OLAP table then convert to visual component</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-10-09-verify-calculation-result-in-olap-table-then-convert-to-visual-component/</link>
      <pubDate>Sat, 09 Oct 2021 09:33:43 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-10-09-verify-calculation-result-in-olap-table-then-convert-to-visual-component/</guid>
      <description>Today is 24th post of this blog series and I am suprised that I can continue to post blog every week. I got some reply from who loves Celonis in the world and it is my fun to continue posting. Celonis is releasing new functionarity year by year and I am interested in catching up them and thinking how these functions help my clients. After listening to next week&amp;rsquo;s Celonis World Tour Webiner in Tokyo, I will get more inspiration to write blog.</description>
    </item>
    
    <item>
      <title>Group similar cases by Clustering</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-10-02-group-similar-cases-by-clustering/</link>
      <pubDate>Sat, 02 Oct 2021 10:56:43 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-10-02-group-similar-cases-by-clustering/</guid>
      <description>Last week I posted Convert Quantitative value to Categorical one by Quantile Function to create categorical dimension. Today I will create categorical dimension via different way, clustering.
Clustering is one of the unsupervised learning method, to automatically group cases by their attributes. Celonis PQL has clustering funciton KMEANS, so you are ready to use clustering.
Today I will use O2C process and would like to group customers by (1) their lead time from Invoice send to Clear Invoice and (2) net value.</description>
    </item>
    
    <item>
      <title>Convert Quantitative value to Categorical one by Quantile Function</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-09-25-convert-quantitative-value-to-categorical-one-by-quantile-function/</link>
      <pubDate>Sat, 25 Sep 2021 09:42:52 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-09-25-convert-quantitative-value-to-categorical-one-by-quantile-function/</guid>
      <description>Last week I posted Integrate Button Dropdown Entries to one Formula and integrated multiple dimensions. At that time I used string column that enable you to categorize each case, that is called categorical variable. In contrast quantitative variable such as order quantity, net value is not normally possible to use for dimension. If you would like to use these column as dimension, you need to convert its value to categorical value.</description>
    </item>
    
    <item>
      <title>Integrate Button Dropdown Entries to one Formula</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-09-18-integrate-button-dropdown-entries-to-one-formula/</link>
      <pubDate>Sat, 18 Sep 2021 10:42:24 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-09-18-integrate-button-dropdown-entries-to-one-formula/</guid>
      <description>I usually use two types of analysis components, time scale graph and drilldown table for my development projects. These compnents makes it possible to discover root cause of target KPIs by changing time scale or drilldown dimension.
In the end, these components and attaching button dropdown were maintained many times, but I was annoyed to set variable value (especially long PQL) to each button entry (it was also cause of defects against my analysis sheet).</description>
    </item>
    
    <item>
      <title>Use BIND function to enable multiple DOMAIN_TABLE</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-09-11-use-bind-function-to-enable-multiple-domain-table/</link>
      <pubDate>Sat, 11 Sep 2021 13:16:26 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-09-11-use-bind-function-to-enable-multiple-domain-table/</guid>
      <description>Variant Explorer is the major Celonis Analytical view to find process pattern by GUI operation. Of course I used it many times and enjoyed it at first, and found that I had a lot of effort to filter on and off to observe process. Today I would like to create collective list of variant to see major process KPI. Also I would like to show you BIND function that is difficult to understand but quite convenient if you know it.</description>
    </item>
    
    <item>
      <title>Convert count unit of KPI by COUNT DISTINCT</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-09-04-convert-count-unit-of-kpi-by-count-distinct/</link>
      <pubDate>Sat, 04 Sep 2021 09:06:11 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-09-04-convert-count-unit-of-kpi-by-count-distinct/</guid>
      <description>Last week I was asked to convert count unit of some KPI (that returns 1 or 0) from delivery item to delivery document (convert if all items in the document are 1 then document KPI is 1, else 0). In this case delivery document and item columns are stored at Activity table like this post&amp;rsquo;s third topic. I already used DOMAIN_TABLE to group activity record by delivery item then calculate KPI.</description>
    </item>
    
    <item>
      <title>Count rows of Tables in various way</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-08-28-count-rows-of-tables-in-various-way/</link>
      <pubDate>Sat, 28 Aug 2021 11:11:45 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-08-28-count-rows-of-tables-in-various-way/</guid>
      <description>Counting rows of tables is frequently used so we are not aware about how to do it. But sometimes I got stuck to do it so I would like to deep dive this topic today.
For this demo case I use P2P data model and three tables, header EKKO (key columns are MANDT/EBELN), item EKPO (MANDT/EBELN/EBELP) and activity. Also I used sum of item Net value. Below screen is the result of this demo.</description>
    </item>
    
    <item>
      <title>Make Conditional Function to return 1 or NULL</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-08-21-make-conditional-function-to-return-1-or-null/</link>
      <pubDate>Sat, 21 Aug 2021 10:42:17 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-08-21-make-conditional-function-to-return-1-or-null/</guid>
      <description>Previously I posted Handle NULL efficiently in Aggregation Function and discussed how to unite formula of COUNT, SUM, AVG functions. Today I will create boolean function and use it for conditional aggregation.
Imagine you would like to calculate net value of Sales Order (VBAP.NETWR_CONVERTED) in O2C process, but there are some conditons to calculate it. First condition is that sales order is active, means rejection reason column (VBAP.ABGRU) is not set.</description>
    </item>
    
    <item>
      <title>Share my Analysis by Content-CLI</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-08-14-share-my-analysis-by-content-cli/</link>
      <pubDate>Sat, 14 Aug 2021 13:33:06 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-08-14-share-my-analysis-by-content-cli/</guid>
      <description>Recently I was thinking how to share my work with you not only blog post but experience by live environment. Today I would like to tell how to reproduce my Analysis to your environment by Content-CLI. Content-CLI is used to copy programs between Celonis environments via backup file. I created backup file and upload to public repository. So you can download it and reproduce my program in your environment.
First you need to open your training environment if you do not.</description>
    </item>
    
    <item>
      <title>Create Matrix of Throughput Time by Pivot Table</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-08-07-create-matrix-of-throughput-time-by-pivot-table/</link>
      <pubDate>Sat, 07 Aug 2021 14:07:40 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-08-07-create-matrix-of-throughput-time-by-pivot-table/</guid>
      <description>At the previous post of Customize Process Explorer, I showed how to determine throughput time between two activities. This KPI is visible when expanding Process Explorer, but font size of Process Explorer become smaller and smaller when expanding connections, so it is difficult to grasp overview of throughput time.
Until previous post, I usually used OLAP table to show the list of KPI value. Of course it is good enough to grasp overview, but today I used different component &amp;lsquo;Pivot Table&amp;rsquo; to show KPI.</description>
    </item>
    
    <item>
      <title>Create Additional Entry to Button Dropdown</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-07-31-create-additional-entry-to-button-dropdown/</link>
      <pubDate>Sat, 31 Jul 2021 10:00:39 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-07-31-create-additional-entry-to-button-dropdown/</guid>
      <description>I looked at the Celopeers post that had issue when variable input is blank (NULL) then PQL using this variable had error.
I already used work around below to skip FILTER execution if variable is null. But I also felt troublesome in two points. First is this is not officially documented so myself need to instruct to my colleagues. Second is more important, I would like to unfilter this selection if variable is not set, but there was no way to do it.</description>
    </item>
    
    <item>
      <title>Categorize and Name Activity</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-07-24-categorize-and-name-activity/</link>
      <pubDate>Sat, 24 Jul 2021 10:47:12 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-07-24-categorize-and-name-activity/</guid>
      <description>In the previous post Transform Source System Tables to Minimize Data Model Tables, I recommended to convert some kind of source system tables to Activity. Today I focus on Activity and would like to share my way how to categorize and name Activity.
First point is to split Activity name to two parts, more general part and detail part. For example, in SAP ECC or S4HANA Order to Cash process, general Activity name is Create Sales Order when data committed in VA01 transaction.</description>
    </item>
    
    <item>
      <title>Transform Source System Tables to Minimize Data Model Tables</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-07-17-transform-source-system-tables-to-minimize-data-model-tables/</link>
      <pubDate>Sat, 17 Jul 2021 13:24:10 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-07-17-transform-source-system-tables-to-minimize-data-model-tables/</guid>
      <description>In the last part of previous post Utilize N-M relationship between Activity and Dimension Tables, I said just mimicing source system&amp;rsquo;s table structure to data model is not useful for Celonis data model. Also I feel loading time to data model is exponentially long when one more table is added. So minimizing data model table is good practice especially for real time analytics.
In case of SAP Order to Cash (O2C) scenario, case table is sales order item (VBAP).</description>
    </item>
    
    <item>
      <title>Utilize N-M relationship between Activity and Dimension Tables</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-07-10-utilize-n-m-relationship-between-activity-and-dimension-tables/</link>
      <pubDate>Sat, 10 Jul 2021 10:08:50 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-07-10-utilize-n-m-relationship-between-activity-and-dimension-tables/</guid>
      <description>When you determine data model structure by yourself, basically you should follow snowflake schema writtern in Understand how Tables are joined in Data Model. From case table perspective, n side is case table, and 1 side is another dimension table. On the other hand, activity table is the first case that 1 side is case table. If you determine second dimension table that 1 side is case table, please note that you can not use both activity and that dimension table at once because those tables are N-M relationship via case table.</description>
    </item>
    
    <item>
      <title>Maintain Saved Formulas effectively</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-07-03-maintain-saved-formulas-effectively/</link>
      <pubDate>Sat, 03 Jul 2021 21:25:28 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-07-03-maintain-saved-formulas-effectively/</guid>
      <description>In the last post Handle NULL efficiently in Aggregation Function, I used saved formulas to split long and complex PQL to reusable components. Today I would like to share my best practice to use saved formulas.
For example I would like to ananlyze throughput time between arbitrary two activities. As below screenshot I created three dropdown buttons, switching time unit (sec, min, hour, day etc.) and two activities (from / to).</description>
    </item>
    
    <item>
      <title>Handle NULL efficiently in Aggregation Function</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-06-26-handle-null-efficiently-in-aggregation-function/</link>
      <pubDate>Sat, 26 Jun 2021 09:37:59 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-06-26-handle-null-efficiently-in-aggregation-function/</guid>
      <description>I looked at many PQLs that can be simplified if they know about NULL handling well. Today I would like to tell how to handle NULL efficiently in Aggregation Functions (COUNT,SUM,AVG etc.).
Today I would like to use O2C process to explain my case. I determined KPI Send Invoice within a day after Ship Goods, because sales company will Ship Goods then should Send Invoice immediately.
Same as previous posts, first I would like to create OLAP table to look into the cases.</description>
    </item>
    
    <item>
      <title>Understand how Tables are joined in Data Model</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-06-19-understand-how-tables-are-joined-in-data-model/</link>
      <pubDate>Sat, 19 Jun 2021 11:42:09 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-06-19-understand-how-tables-are-joined-in-data-model/</guid>
      <description>In the Understand Difference between Dimension and KPI post, I mentioned that Columns used as dimensions and KPIs are inplicitly joined based on data model, but I did not mention how to join tables in data model. Today I would like to say about it.
Celonis Data Model support snowflake schema that consists of multiple tables, each table pair join as 1:N relationship. Normally activity table is top of N side and dimension tables including case table are bottom of 1 side as pyramid hierarchy.</description>
    </item>
    
    <item>
      <title>Calculate Multi Dimensional KPIs</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-06-12-calculate-multi-dimentional-kpis/</link>
      <pubDate>Sat, 12 Jun 2021 13:20:53 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-06-12-calculate-multi-dimentional-kpis/</guid>
      <description>In the last post Recognize Record to Calculate KPI, I showed issue of count duplication when I merged two different dimentional KPIs. Today I will tell how to calculate it correctly.
I will start from changing OLAP table to hide USER_TYPE and ACTIVITY_EN for calculating KPIs grouped by customer master. And I filtered by customer K1 as previous post. Finally I added SUM function to both Rework Time and Reminder Time.</description>
    </item>
    
    <item>
      <title>Recognize Record to Calculate KPI</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-06-05-recognize-record-to-calculate-kpi/</link>
      <pubDate>Sat, 05 Jun 2021 17:34:46 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-06-05-recognize-record-to-calculate-kpi/</guid>
      <description>Using Pull up function, you can calculate various kind of KPIs. I would like to tell in this post is taking care the record (dimensions) to calculate each KPI especially when you unite multiple KPIs to same component.
In this example, I will use Order to Cash process again and first I would like to estiamate time of rework. If rework Activity is operated by manual user, this rework is estimated 1 minute for example.</description>
    </item>
    
    <item>
      <title>Use Pull up function as Subquery</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-05-29-use-pull-up-function-as-subquery/</link>
      <pubDate>Sat, 29 May 2021 13:04:06 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-05-29-use-pull-up-function-as-subquery/</guid>
      <description>In the last post Determine First Time Right by Pull up function, I broke down PQL of FTR step by step. Today I will explain more complex KPI in similar way. By the way you may know Subquery that enables to pass result of SQL to parts of another SQL. PQL can also do similar things by Pull up function.
In this post, for example I would like to calculate Average rework count after Delivery in Order to Cash process, and definition of rework is same as previous post.</description>
    </item>
    
    <item>
      <title>Determine First Time Right by Pull up function</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-05-22-determine-first-time-right-by-pull-up-function/</link>
      <pubDate>Sat, 22 May 2021 16:31:16 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-05-22-determine-first-time-right-by-pull-up-function/</guid>
      <description>As a first example of Pull up function, I will show you First Time Right (FTR) that means process without rework activities (link).
To determine whether each case is FTR or not, first I should determine rework activities, then judge if each case has rework activities or not. In this example I determined that rework activity name is started from Change. PQL to find such a string pattern is Change %, using wildcard % after Change.</description>
    </item>
    
    <item>
      <title>Understand mechanism of Pull up function</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-05-15-understand-mechanism-of-pull-up-function/</link>
      <pubDate>Sat, 15 May 2021 10:00:14 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-05-15-understand-mechanism-of-pull-up-function/</guid>
      <description>Pull up aggregation function, called PU function, like PU_COUNT, PU_SUM, PU_AVG, PU_MAX, PU_MIN looks similar to Standard aggregation function like COUNT, SUM, AVG, MAX, MIN. In Celonis, it is important to understand that output of PU function is not KPI but dimension. You can see Understand Difference between Dimension and KPI.
If you know about SQL, you can imagine that PU function is similar to window function in SQL. As other functions, PU functions add dynamic column in grouping table.</description>
    </item>
    
    <item>
      <title>Customize Process Explorer</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-05-08-customize-process-explorer/</link>
      <pubDate>Sat, 08 May 2021 09:15:03 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-05-08-customize-process-explorer/</guid>
      <description>Process explorer in Celonis Analysis is default application to analyze process. First you can see main process pattern (called happy path), count of each step (activity) and count of path between two steps (connection). You can show minor activities and connections, or switch throughput time instead of count of connection.
I think there are two ways to customize Process explorer.
 Change activity name Add more KPI against activity / connection  Change activity name If you want to add detail information to each activity, you can change activity dimension.</description>
    </item>
    
    <item>
      <title>Understand Difference between Dimension and KPI</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-05-01-understand-difference-between-dimension-and-kpi/</link>
      <pubDate>Sat, 01 May 2021 15:25:08 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-05-01-understand-difference-between-dimension-and-kpi/</guid>
      <description>If you are beginner of creating Celonis Analysis, I believe you will first create OLAP table to understand how Celonis PQL works with data model. Points are,
 Dimensions are grouping key to calculate KPI, normally they are string (character) columns KPIs are calculated figures based on specific numeric columns, or count of rows of any columns, it must be numeric value in either cases Columns used as dimensions and KPIs are inplicitly joined based on data model  Below screenshot is the quite simple example to show Dimension, KPI, and data model.</description>
    </item>
    
  </channel>
</rss>