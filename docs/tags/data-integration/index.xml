<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Integration on Celonis hacks</title>
    <link>https://kaztakata.github.io/celonis-hacks/tags/data-integration/</link>
    <description>Recent content in Data Integration on Celonis hacks</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 04 Jun 2022 09:25:43 +0900</lastBuildDate><atom:link href="https://kaztakata.github.io/celonis-hacks/tags/data-integration/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Find out HTTP Request from GUI Function</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-06-04-find-out-http-request-from-gui-function/</link>
      <pubDate>Sat, 04 Jun 2022 09:25:43 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-06-04-find-out-http-request-from-gui-function/</guid>
      <description>In the previous post Observe HTTP request in Pycelonis login script, I showed how to observe HTTP request under Pycelonis API. In this observation I found HTTP request requires at least Authorization header and of course URL to reach to resource in Celonis EMS. Also I found that I can manage to investigate which HTTP request is sent when calling Pycelonis class method.
By the way, not all HTTP requests are implemented in Pycelonis.</description>
    </item>
    
    <item>
      <title>Execute Periodic ETL Automatically</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-04-30-execute-periodic-etl-automatically/</link>
      <pubDate>Sat, 30 Apr 2022 08:54:10 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-04-30-execute-periodic-etl-automatically/</guid>
      <description>Until last post Validate Data Model by Studio Analysis, I completed creating ETL programs in Data Integration. But this is still test product because this programs are executed manually by operator. In this post, I would like to share the final piece of Data Integration, how to run ETL programs automatically after production release.
There are several ways to achieve automatic ETL, for example in my production system I am using scheduler in Machine Learning Workbench to trigger Jupyter Notebook, then operate Data Integration via Pycelonis (Python API for Celonis EMS).</description>
    </item>
    
    <item>
      <title>Validate Data Model by Studio Analysis</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-04-23-validate-data-model-by-studio-analysis/</link>
      <pubDate>Sat, 23 Apr 2022 08:43:22 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-04-23-validate-data-model-by-studio-analysis/</guid>
      <description>At last post Construct My First Data Model, I created Data Model and load data to it. Normally initial load is not perfect, so I should check data in Data Model. Today I would like to share how to validate my Data Model using Analysis. By the way, Celonis EMS main function is now Studio (and App for viewer) and Analysis is also part of Studio, so today I will create Studio instead of Process Analytics to create Analysis.</description>
    </item>
    
    <item>
      <title>Construct My First Data Model</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-04-16-construct-my-first-data-model/</link>
      <pubDate>Sat, 16 Apr 2022 11:03:14 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-04-16-construct-my-first-data-model/</guid>
      <description>Today I would like to create draft version of data model then complete series of ETL (Extraction, Transformation, Load) started from last November.
As I showed in Adjust Time Zone of Event Time in Global Transformation, I created global VIEW for activity table. Before creating Data Model, I would like to create one more global VIEW against case table. As below SQL, it is quite simple to copy all columns of issues in Planio connection, plus one more column _CASE_KEY that is converting from interger id to character.</description>
    </item>
    
    <item>
      <title>Inspect Table Data by SELECT statement</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-04-09-inspect-table-data-by-select-statement/</link>
      <pubDate>Sat, 09 Apr 2022 17:40:32 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-04-09-inspect-table-data-by-select-statement/</guid>
      <description>Today I would like to share small tips in transformation tasks. Of course main purpose of transforamtion is to create event log table etc. but we are not always sure about source system behavior, so before completing transformation SQL I must investigate data of source system. As I already showed, I can use simple SELECT statement to inspect tables.
Previously when I was familiar with SAP ECC, the only way to easily inspect SAP table is to download table data by SE16 transaction and open Microsoft Excel and simply filter data or use pivot table etc.</description>
    </item>
    
    <item>
      <title>Adjust Time Zone of Event Time in Global Transformation</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-04-02-adjust-time-zone-of-event-time-in-global-transformation/</link>
      <pubDate>Sat, 02 Apr 2022 16:08:17 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-04-02-adjust-time-zone-of-event-time-in-global-transformation/</guid>
      <description>Until now I do not care about value of event time, that is stored at source system like Planio. If I analyse only one system like my current project it is no problem, but if I would like to analyze data from multiple source systems, I should take care time zone of each system.
In my case, I previously analyzed multiple SAP ECC systems, those are located in various area of world (US, EU, JP etc.</description>
    </item>
    
    <item>
      <title>Handle Day based Activity as Milestone</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-03-26-handle-day-based-activity-as-milestone/</link>
      <pubDate>Sat, 26 Mar 2022 01:48:53 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-03-26-handle-day-based-activity-as-milestone/</guid>
      <description>I already created four activities until last post Unite SQL statements by CASE Expression, those are fillfilled requirement of event log. Going back to Consider Case ID before Starting Transformation, case ID is the biggest requirement. Also is is not so big as case ID, but event time is important too. In process mining, event time should be year, month, day plus hour, minute, second (YYYY-MM-DD HH:MI:SS in Vertica format). I guess event time in process mining referred to that is recorded automatically by system responding to something action.</description>
    </item>
    
    <item>
      <title>Unite SQL statements by CASE Expression</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-03-19-unite-sql-statements-by-case-expression/</link>
      <pubDate>Sat, 19 Mar 2022 09:14:35 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-03-19-unite-sql-statements-by-case-expression/</guid>
      <description>Until Split Long SQL Using Views post I created four activities and each SQLs, and I found three of four have same table join pattern. So I created VIEW to shorten JOIN predicate for each SQL. It is nice to shorten total statement volume but almost all statement except for JOIN predicate is duplicated among SQLs. When something change is required, maintenance of each SQLs is annoying work. Today I would like to integrate SQLs using CASE expression.</description>
    </item>
    
    <item>
      <title>Split Long SQL Using Views</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-03-12-split-long-sql-using-views/</link>
      <pubDate>Sat, 12 Mar 2022 16:28:51 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-03-12-split-long-sql-using-views/</guid>
      <description>At last post I wrote Compose Activity from Joining Multiple Tables to create second activity Close Issue. Final version of SQL statement was long even I just used three tables. In real process mining project I handled hundred of tables and wrote quite long SQLs. At that time I faced same patterns of SQL in multiple activities. So I introduces VIEW in my SQLs for grouping same pattern of SELECT SQL, similar to create function (method) in programming.</description>
    </item>
    
    <item>
      <title>Compose Activity from Joining Multiple Tables</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-03-05-compose-activity-from-joining-multiple-tables/</link>
      <pubDate>Sat, 05 Mar 2022 08:30:59 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-03-05-compose-activity-from-joining-multiple-tables/</guid>
      <description>In the last post of Insert Simple Record into Activity Table, I created SQL to insert Raise Issue activity. That SQL was simple because only one table issues are used as data source. Today I will create SQL of Close Issue activity that requires multiple tables.
Same as previous post, first I should analyze columns to compose Close Issue activity. This is happend when I change Status column in each issue, so I need change history.</description>
    </item>
    
    <item>
      <title>Insert Simple Record into Activity Table</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-02-26-insert-simple-record-into-activity-table/</link>
      <pubDate>Sat, 26 Feb 2022 09:14:12 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-02-26-insert-simple-record-into-activity-table/</guid>
      <description>In the last post of Determine Process Mining Tables based on Project Goal, I talked strategy of transforming Planio data to event log. Then I wrote SQL to create activity table.
From now I would like to insert activity record to that table. Based on the discussion of last post, at least I need two activities Raise Issue and Close Issue to measure throughput time between them. That is the next step to achieve my goal.</description>
    </item>
    
    <item>
      <title>Determine Process Mining Tables based on Project Goal</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-02-19-determine-process-mining-tables-based-on-project-goal/</link>
      <pubDate>Sat, 19 Feb 2022 13:43:41 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-02-19-determine-process-mining-tables-based-on-project-goal/</guid>
      <description>I talked about how to consider Case ID in advance in last post. That is same as goal setting of process mining project, it means what would like to be measured in process mining and why. Case is the unit of measuring performance and grouping activities.
Considering my case of Planio, for example I would like to measure 1) Throughput time from raising issue to closing it, 2) How many users are involved until closing issue.</description>
    </item>
    
    <item>
      <title>Consider Case ID before Starting Transformation</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-02-12-consider-case-id-before-starting-transformation/</link>
      <pubDate>Sat, 12 Feb 2022 13:03:00 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-02-12-consider-case-id-before-starting-transformation/</guid>
      <description>After long explanation of Extractor Builder, I can move to Transformation Topic from now, using Planio issue and change history. But before starting detail discussion, I would like to discuss general issues at first.
In process mining context, Transformation is procedure to generate event log table from source tables. Event Log or Event Data is the collection of case and its event (activity) with timestamp. Case ID can be associated with multiple activities in source system, in other word it is not possible to generate event log without case ID.</description>
    </item>
    
    <item>
      <title>Tune Endpoint Parameter Relevant to Delta Load</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-02-05-tune-endpoint-parameter-relevant-to-delta-load/</link>
      <pubDate>Sat, 05 Feb 2022 10:05:51 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-02-05-tune-endpoint-parameter-relevant-to-delta-load/</guid>
      <description>Until last post Setup Dependent Endpoint in Extractor Builder, I prepared endpoints of both Planio Issues and their journals. Today I would like to tackle final setup of extractor to deal with Delta Load option.
Referring to the Planio Documentation, updated_on column exists for filtering Issues. This timestamp column is updated when creating and updating relevant issue, so it is appropriate column for Delta Load. Open Celonis Extractor builder then go to 4 Define Endpoints.</description>
    </item>
    
    <item>
      <title>Setup Dependent Endpoint in Extractor Builder</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-01-29-setup-dependent-endpoint-in-extractor-builder/</link>
      <pubDate>Sat, 29 Jan 2022 09:17:16 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-01-29-setup-dependent-endpoint-in-extractor-builder/</guid>
      <description>In the last post Configure Endpoint for Suitable Extraction, I configured Endpoint in Extractor Builder to suit my business requirements, and still there are points to extract change history of issues, and to extract data by Delta Load option. Today I would like to setup regarding change history using Dependent Endpoint in Extractor Builder.
At first how do I extract change history of Planio Issue ? Again I looked at Planio Documentation and found I can get single issue with journals (meaning change history in Planio).</description>
    </item>
    
    <item>
      <title>Configure Endpoint for Suitable Extraction</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-01-22-configure-endpoint-for-suitable-extraction/</link>
      <pubDate>Sat, 22 Jan 2022 08:08:39 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-01-22-configure-endpoint-for-suitable-extraction/</guid>
      <description>In the last post Connect to Source System via REST API, I shared how to set up Extractor Builder and extracted Issue from Planio. It was shortest path to be avaiable for extraction job, so it is not enough for production job. Today I would like to configure Endpoint in Extractor Builder to resolve problems I experienced.
First problem I faced is upper limit of extraction data. Some day I found that I could not get issue record until 25.</description>
    </item>
    
    <item>
      <title>Connect to Source System via REST API</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-01-15-connect-to-source-system-via-rest-api/</link>
      <pubDate>Sat, 15 Jan 2022 10:27:09 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-01-15-connect-to-source-system-via-rest-api/</guid>
      <description>At previous post Prepare Source System to Generate Event Log, I prepared Planio as source system for this blog, and entered few events (create Issue, update Issue Status) to it. Now it is time to extract event log from Planio. As other SaaS solution do, Planio also has REST API to extract data from outside. Currently Celonis EMS has ability to extract from arbitrary system that has REST API, Extractor Builder.</description>
    </item>
    
    <item>
      <title>Prepare Source System to Generate Event Log</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-01-08-prepare-source-system-to-generate-event-log/</link>
      <pubDate>Sat, 08 Jan 2022 16:35:30 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-01-08-prepare-source-system-to-generate-event-log/</guid>
      <description>Until last post I explained extraction topics using own Postgres database. I think it is better to test extraction functions with changing database by yourself. But it is hard to manually input data record that is meaningful as event log.
From now on I will move to transformation topic. To explain this, I think it is required to prepare source system that has user interface, database and API to connect to Celonis EMS, to easily generate event log and extract it.</description>
    </item>
    
    <item>
      <title>Pay attention to Extract SAP Tables</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-12-25-pay-attention-to-extract-sap-tables/</link>
      <pubDate>Sat, 25 Dec 2021 08:53:48 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-12-25-pay-attention-to-extract-sap-tables/</guid>
      <description>Until last post I explained general topics of extraction task, adapted to all kind of source systems. Today I would like to focus on SAP ECC or S4HANA as source system and would like to tell you the SAP specific issues.
First issue is regarding source system itself. We would like to guarantee source system&amp;rsquo;s availability even if I connect Celonis EMS to that. So we may choose testing environment that is snapshot of production system, as source system that connect to Celonis EMS.</description>
    </item>
    
    <item>
      <title>Use Pseudonymized Column as Grouping Key</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-12-18-use-pseudonymized-column-as-grouping-key/</link>
      <pubDate>Sat, 18 Dec 2021 09:22:56 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-12-18-use-pseudonymized-column-as-grouping-key/</guid>
      <description>One of the biggest headache for data engineer like me is how to assure data security when extracting data. Especially personal information should be dealt sensitively, otherwise I may be punished by each region&amp;rsquo;s law (e.g. GDPR).
When I operate Celonis EMS, I try not to extract sensitive information from the beginning, for example I do not extract table of customer address (ADRC table in SAP etc.). But this information is sometimes effective for grouping key of counting case etc.</description>
    </item>
    
    <item>
      <title>Understand Delta Load Configuration Difference in Adding Column Scenario</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-12-11-understand-delta-load-configuration-difference-in-adding-column-scenario/</link>
      <pubDate>Sat, 11 Dec 2021 21:54:02 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-12-11-understand-delta-load-configuration-difference-in-adding-column-scenario/</guid>
      <description>Last time I showed behavior when I added new record then extracted that record by Delta Load (Verify Cloning Table Contents via Delta Load). Delta Load is effective way to minimize extraction effort, but it is not always applied. Today, it is continued from previous post, I would like to add column to cloned table and observe behavior of extraction task.
After starting system operation including database, normally system is changing its requirement and extend function and database etc.</description>
    </item>
    
    <item>
      <title>Verify Cloning Table Contents via Delta Load</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-12-04-verify-cloning-table-contents-via-delta-load/</link>
      <pubDate>Sat, 04 Dec 2021 12:49:44 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-12-04-verify-cloning-table-contents-via-delta-load/</guid>
      <description>Following last week&amp;rsquo;s Minimize Extraction Time by Delta Load Option, today I would like to insert new record to Postgres table then try Delta Load again to extract it. To do this, I will start from operating pgAdmin, that is already ready for my loal machine after docker-compose.
First step is to enter localhost:5050 to my browser, then at the login screen enter pgadmin@celonis.cloud as email and pgadmin as password then click login button.</description>
    </item>
    
    <item>
      <title>Minimize Extraction Time by Delta Load Option</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-11-27-minimize-extraction-time-by-delta-load-option/</link>
      <pubDate>Sat, 27 Nov 2021 09:51:19 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-11-27-minimize-extraction-time-by-delta-load-option/</guid>
      <description>Last week I extracted Postgres table and looked at the log to understand mechanism of data transfer. At that time I used Full Load option to extract data, that is to replace all table contents and schema to latest version. That is easiest way to synchronize tables between source system (Postgres) and Celonis, but it takes a lot of time to complete this task. So that I should also use second option Delta Load to minimize extraction time.</description>
    </item>
    
    <item>
      <title>Look at Data Transfer Process by Data Job Log</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-11-20-look-at-data-transfer-process-by-data-job-log/</link>
      <pubDate>Sat, 20 Nov 2021 08:39:24 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-11-20-look-at-data-transfer-process-by-data-job-log/</guid>
      <description>Last week I posted Connect to Celonis and Bring Back Instruction to look at how Extractor works to connect between Celonis and Postgres. This week I would like to extract data from Postgres and look at data transfer process by data job log.
In the Data Integration, I create new Data Job with Data Connection I created last week, then create new extraction task. In the next screen I add new table public.</description>
    </item>
    
    <item>
      <title>Connect to Celonis and Bring Back Instruction</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-11-13-connect-to-celonis-and-bring-back-instruction/</link>
      <pubDate>Sat, 13 Nov 2021 10:19:49 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-11-13-connect-to-celonis-and-bring-back-instruction/</guid>
      <description>From last week I started Data Integration series and posted Run Extractor on Your Local Machine to prepare for my Extractor and Postgres database. Today I will start using Extractor and show you the mechanism to extract data safely.
From this week I will start Extractor and Postgres by next two steps (same as last week).
 Open VS code and open terminal at celonis-postgres folder. Enter docker-compose up in VS code terminal.</description>
    </item>
    
    <item>
      <title>Run Extractor on Your Local Machine</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-11-06-run-extractor-on-your-local-machine/</link>
      <pubDate>Sat, 06 Nov 2021 09:59:30 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-11-06-run-extractor-on-your-local-machine/</guid>
      <description>From this week I would like to explain my experience regarding Data Integration functions (Extraction, Transformation, Load etc.). To do this, I try to create sample source systems and build code in Celonis training environment.
As first topic, I would like to explain on premise Extractor, that is in the middle between your source systems (SAP, Oracle etc.) and Celonis EMS and support transferring data. By the way, because I do not want to pay licence of source systems for this blog, I would like to use open source Postgres database.</description>
    </item>
    
    <item>
      <title>Categorize and Name Activity</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-07-24-categorize-and-name-activity/</link>
      <pubDate>Sat, 24 Jul 2021 10:47:12 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-07-24-categorize-and-name-activity/</guid>
      <description>In the previous post Transform Source System Tables to Minimize Data Model Tables, I recommended to convert some kind of source system tables to Activity. Today I focus on Activity and would like to share my way how to categorize and name Activity.
First point is to split Activity name to two parts, more general part and detail part. For example, in SAP ECC or S4HANA Order to Cash process, general Activity name is Create Sales Order when data committed in VA01 transaction.</description>
    </item>
    
    <item>
      <title>Transform Source System Tables to Minimize Data Model Tables</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-07-17-transform-source-system-tables-to-minimize-data-model-tables/</link>
      <pubDate>Sat, 17 Jul 2021 13:24:10 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-07-17-transform-source-system-tables-to-minimize-data-model-tables/</guid>
      <description>In the last part of previous post Utilize N-M relationship between Activity and Dimension Tables, I said just mimicing source system&amp;rsquo;s table structure to data model is not useful for Celonis data model. Also I feel loading time to data model is exponentially long when one more table is added. So minimizing data model table is good practice especially for real time analytics.
In case of SAP Order to Cash (O2C) scenario, case table is sales order item (VBAP).</description>
    </item>
    
  </channel>
</rss>
