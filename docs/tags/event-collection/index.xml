<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Event Collection on Celonis hacks</title>
    <link>https://kaztakata.github.io/celonis-hacks/tags/event-collection/</link>
    <description>Recent content in Event Collection on Celonis hacks</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 27 Nov 2021 09:51:19 +0900</lastBuildDate>
    
	<atom:link href="https://kaztakata.github.io/celonis-hacks/tags/event-collection/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Minimize Extraction Time by Delta Load Option</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-11-27-minimize-extraction-time-by-delta-load-option/</link>
      <pubDate>Sat, 27 Nov 2021 09:51:19 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-11-27-minimize-extraction-time-by-delta-load-option/</guid>
      <description>Last week I extracted Postgres table and looked at the log to understand mechanism of data transfer. At that time I used Full Load option to extract data, that is to replace all table contents and schema to latest version. That is easiest way to synchronize tables between source system (Postgres) and Celonis, but it takes a lot of time to complete this task. So that I should also use second option Delta Load to minimize extraction time.</description>
    </item>
    
    <item>
      <title>Look at Data Transfer Process by Data Job Log</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-11-20-look-at-data-transfer-process-by-data-job-log/</link>
      <pubDate>Sat, 20 Nov 2021 08:39:24 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-11-20-look-at-data-transfer-process-by-data-job-log/</guid>
      <description>Last week I posted Connect to Celonis and Bring Back Instruction to look at how Extractor works to connect between Celonis and Postgres. This week I would like to extract data from Postgres and look at data transfer process by data job log.
In the Event Collection, I create new Data Job with Data Connection I created last week, then create new extraction task. In the next screen I add new table public.</description>
    </item>
    
    <item>
      <title>Connect to Celonis and Bring Back Instruction</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-11-13-connect-to-celonis-and-bring-back-instruction/</link>
      <pubDate>Sat, 13 Nov 2021 10:19:49 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-11-13-connect-to-celonis-and-bring-back-instruction/</guid>
      <description>From last week I started Event Collection series and posted Run Extractor on Your Local Machine to prepare for my Extractor and Postgres database. Today I will start using Extractor and show you the mechanism to extract data safely.
From this week I will start Extractor and Postgres by next two steps (same as last week).
 Open VS code and open terminal at celonis-postgres folder. Enter docker-compose up in VS code terminal.</description>
    </item>
    
    <item>
      <title>Run Extractor on Your Local Machine</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-11-06-run-extractor-on-your-local-machine/</link>
      <pubDate>Sat, 06 Nov 2021 09:59:30 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-11-06-run-extractor-on-your-local-machine/</guid>
      <description>From this week I would like to explain my experience regarding Event Collection functions (Extraction, Transformation, Load etc.). To do this, I try to create sample source systems and build code in Celonis training environment.
As first topic, I would like to explain on premise Extractor, that is in the middle between your source systems (SAP, Oracle etc.) and Celonis EMS and support transferring data. By the way, because I do not want to pay licence of source systems for this blog, I would like to use open source Postgres database.</description>
    </item>
    
    <item>
      <title>Categorize and Name Activity</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-07-24-categorize-and-name-activity/</link>
      <pubDate>Sat, 24 Jul 2021 10:47:12 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-07-24-categorize-and-name-activity/</guid>
      <description>In the previous post Transform Source System Tables to Minimize Data Model Tables, I recommended to convert some kind of source system tables to Activity. Today I focus on Activity and would like to share my way how to categorize and name Activity.
First point is to split Activity name to two parts, more general part and detail part. For example, in SAP ECC or S4HANA Order to Cash process, general Activity name is Create Sales Order when data committed in VA01 transaction.</description>
    </item>
    
    <item>
      <title>Transform Source System Tables to Minimize Data Model Tables</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-07-17-transform-source-system-tables-to-minimize-data-model-tables/</link>
      <pubDate>Sat, 17 Jul 2021 13:24:10 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-07-17-transform-source-system-tables-to-minimize-data-model-tables/</guid>
      <description>In the last part of previous post Utilize N-M relationship between Activity and Dimension Tables, I said just mimicing source system&amp;rsquo;s table structure to data model is not useful for Celonis data model. Also I feel loading time to data model is exponentially long when one more table is added. So minimizing data model table is good practice especially for real time analytics.
In case of SAP Order to Cash (O2C) scenario, case table is sales order item (VBAP).</description>
    </item>
    
  </channel>
</rss>