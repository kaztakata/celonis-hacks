<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Transformation on Celonis hacks</title>
    <link>https://kaztakata.github.io/celonis-hacks/tags/transformation/</link>
    <description>Recent content in Transformation on Celonis hacks</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 26 Feb 2022 09:14:12 +0900</lastBuildDate>
    
	<atom:link href="https://kaztakata.github.io/celonis-hacks/tags/transformation/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Insert Simple Record into Activity Table</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-02-26-insert-simple-record-into-activity-table/</link>
      <pubDate>Sat, 26 Feb 2022 09:14:12 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-02-26-insert-simple-record-into-activity-table/</guid>
      <description>In the last post of Determine Process Mining Tables based on Project Goal, I talked strategy of transforming Planio data to event log. Then I wrote SQL to create activity table.
From now I would like to insert activity record to that table. Based on the discussion of last post, at least I need two activities Raise Issue and Close Issue to measure throughput time between them. That is the next step to achieve my goal.</description>
    </item>
    
    <item>
      <title>Determine Process Mining Tables based on Project Goal</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-02-19-determine-process-mining-tables-based-on-project-goal/</link>
      <pubDate>Sat, 19 Feb 2022 13:43:41 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-02-19-determine-process-mining-tables-based-on-project-goal/</guid>
      <description>I talked about how to consider Case ID in advance in last post. That is same as goal setting of process mining project, it means what would like to be measured in process mining and why. Case is the unit of measuring performance and grouping activities.
Considering my case of Planio, for example I would like to measure 1) Throughput time from raising issue to closing it, 2) How many users are involved until closing issue.</description>
    </item>
    
    <item>
      <title>Consider Case ID before Starting Transformation</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-02-12-consider-case-id-before-starting-transformation/</link>
      <pubDate>Sat, 12 Feb 2022 13:03:00 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-02-12-consider-case-id-before-starting-transformation/</guid>
      <description>After long explanation of Extractor Builder, I can move to Transformation Topic from now, using Planio issue and change history. But before starting detail discussion, I would like to discuss general issues at first.
In process mining context, Transformation is procedure to generate event log table from source tables. Event Log or Event Data is the collection of case and its event (activity) with timestamp. Case ID can be associated with multiple activities in source system, in other word it is not possible to generate event log without case ID.</description>
    </item>
    
    <item>
      <title>Prepare Source System to Generate Event Log</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2022-01-08-prepare-source-system-to-generate-event-log/</link>
      <pubDate>Sat, 08 Jan 2022 16:35:30 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2022-01-08-prepare-source-system-to-generate-event-log/</guid>
      <description>Until last post I explained extraction topics using own Postgres database. I think it is better to test extraction functions with changing database by yourself. But it is hard to manually input data record that is meaningful as event log.
From now on I will move to transformation topic. To explain this, I think it is required to prepare source system that has user interface, database and API to connect to Celonis EMS, to easily generate event log and extract it.</description>
    </item>
    
    <item>
      <title>Use Pseudonymized Column as Grouping Key</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-12-18-use-pseudonymized-column-as-grouping-key/</link>
      <pubDate>Sat, 18 Dec 2021 09:22:56 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-12-18-use-pseudonymized-column-as-grouping-key/</guid>
      <description>One of the biggest headache for data engineer like me is how to assure data security when extracting data. Especially personal information should be dealt sensitively, otherwise I may be punished by each region&amp;rsquo;s law (e.g. GDPR).
When I operate Celonis EMS, I try not to extract sensitive information from the beginning, for example I do not extract table of customer address (ADRC table in SAP etc.). But this information is sometimes effective for grouping key of counting case etc.</description>
    </item>
    
    <item>
      <title>Understand Delta Load Configuration Difference in Adding Column Scenario</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-12-11-understand-delta-load-configuration-difference-in-adding-column-scenario/</link>
      <pubDate>Sat, 11 Dec 2021 21:54:02 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-12-11-understand-delta-load-configuration-difference-in-adding-column-scenario/</guid>
      <description>Last time I showed behavior when I added new record then extracted that record by Delta Load (Verify Cloning Table Contents via Delta Load). Delta Load is effective way to minimize extraction effort, but it is not always applied. Today, it is continued from previous post, I would like to add column to cloned table and observe behavior of extraction task.
After starting system operation including database, normally system is changing its requirement and extend function and database etc.</description>
    </item>
    
    <item>
      <title>Verify Cloning Table Contents via Delta Load</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-12-04-verify-cloning-table-contents-via-delta-load/</link>
      <pubDate>Sat, 04 Dec 2021 12:49:44 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-12-04-verify-cloning-table-contents-via-delta-load/</guid>
      <description>Following last week&amp;rsquo;s Minimize Extraction Time by Delta Load Option, today I would like to insert new record to Postgres table then try Delta Load again to extract it. To do this, I will start from operating pgAdmin, that is already ready for my loal machine after docker-compose.
First step is to enter localhost:5050 to my browser, then at the login screen enter pgadmin@celonis.cloud as email and pgadmin as password then click login button.</description>
    </item>
    
    <item>
      <title>Minimize Extraction Time by Delta Load Option</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-11-27-minimize-extraction-time-by-delta-load-option/</link>
      <pubDate>Sat, 27 Nov 2021 09:51:19 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-11-27-minimize-extraction-time-by-delta-load-option/</guid>
      <description>Last week I extracted Postgres table and looked at the log to understand mechanism of data transfer. At that time I used Full Load option to extract data, that is to replace all table contents and schema to latest version. That is easiest way to synchronize tables between source system (Postgres) and Celonis, but it takes a lot of time to complete this task. So that I should also use second option Delta Load to minimize extraction time.</description>
    </item>
    
    <item>
      <title>Categorize and Name Activity</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-07-24-categorize-and-name-activity/</link>
      <pubDate>Sat, 24 Jul 2021 10:47:12 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-07-24-categorize-and-name-activity/</guid>
      <description>In the previous post Transform Source System Tables to Minimize Data Model Tables, I recommended to convert some kind of source system tables to Activity. Today I focus on Activity and would like to share my way how to categorize and name Activity.
First point is to split Activity name to two parts, more general part and detail part. For example, in SAP ECC or S4HANA Order to Cash process, general Activity name is Create Sales Order when data committed in VA01 transaction.</description>
    </item>
    
    <item>
      <title>Transform Source System Tables to Minimize Data Model Tables</title>
      <link>https://kaztakata.github.io/celonis-hacks/posts/2021-07-17-transform-source-system-tables-to-minimize-data-model-tables/</link>
      <pubDate>Sat, 17 Jul 2021 13:24:10 +0900</pubDate>
      
      <guid>https://kaztakata.github.io/celonis-hacks/posts/2021-07-17-transform-source-system-tables-to-minimize-data-model-tables/</guid>
      <description>In the last part of previous post Utilize N-M relationship between Activity and Dimension Tables, I said just mimicing source system&amp;rsquo;s table structure to data model is not useful for Celonis data model. Also I feel loading time to data model is exponentially long when one more table is added. So minimizing data model table is good practice especially for real time analytics.
In case of SAP Order to Cash (O2C) scenario, case table is sales order item (VBAP).</description>
    </item>
    
  </channel>
</rss>